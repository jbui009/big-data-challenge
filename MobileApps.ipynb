{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileApps",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVxnJsqBFfb5axAPeAQr7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbui009/big-data-challenge/blob/main/MobileApps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRGxk2yXcJio",
        "outputId": "37e351d7-8179-4e98-9553-2777a880bf68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 0 B/88.7 kB \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [806 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,596 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,474 kB]\n",
            "Get:18 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.8 kB]\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [930 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,827 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [936 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,252 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,035 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [840 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [42.8 kB]\n",
            "Fetched 15.1 MB in 10s (1,487 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies for importing/connecting to SQLalchemy"
      ],
      "metadata": {
        "id": "O8v_GDsbNB8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "S0_erwdGGGEt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a connection"
      ],
      "metadata": {
        "id": "ASA_yuZwNJqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONN = f\"postgresql+psycopg2://postgres:lGtQrtKVXnyU9Fms9r8K@aws-reviews.cephpaqhv83w.us-east-2.rds.amazonaws.com:5432/reviews\"\n",
        "# database connection\n",
        "engine = create_engine(CONN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByeTPePAM-fp",
        "outputId": "f12f97f7-c538-4147-ca51-dc02172b2553"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwke1_ZOg9IV",
        "outputId": "e581bfc2-3be7-4ff6-d98a-21e84ded0e0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-01 01:22:38--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-03-01 01:22:38 (5.98 MB/s) - ‘postgresql-42.2.9.jar’ saved [914037/914037]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mTVVV11YhLdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"MobileAppsCloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "BDv4fEsziweu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Mobile_Apps_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Mobile_Apps_v1_00.tsv.gz\"), sep=\"\\t\", header=True)\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5FwYrmkiay_",
        "outputId": "0d10988a-a5e3-4dfc-a289-00d9d41c22da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+--------------------+-----------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|review_headline|         review_body|review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+--------------------+-----------+\n",
            "|         US|    1774101|R3PMQXEVYR4ZT1|B00DVKOYBM|     263046246|         8 Ball Pool|     Mobile_Apps|          3|            0|          0|   N|                Y|            Fun|It is a fun game ...| 2015-08-31|\n",
            "|         US|   13679234|R1I1K3EO98EMUV|B00R1IA4RS|     128868854|Christmas Cookie ...|     Mobile_Apps|          5|            0|          0|   N|                Y|     Five Stars|Fun cute game.  S...| 2015-08-31|\n",
            "|         US|   15056632|R3K23URSI2MY42|B01069LGHK|     955481764|Relax Melodies Pr...|     Mobile_Apps|          5|            4|          4|   N|                Y|    Great app !|It's a very nice ...| 2015-08-31|\n",
            "|         US|   52477711| RH2P770T10X38|B00X64ZKM0|     455029830|  Star Trek™ Trexels|     Mobile_Apps|          5|            0|          0|   N|                Y|        awesome|I am a huge star ...| 2015-08-31|\n",
            "|         US|   15021042| RV9YBAW8CWFYN|B00KIZO238|     685358159|      Weather Friend|     Mobile_Apps|          1|            0|          0|   N|                Y| Don't like it.|I get better info...| 2015-08-31|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+---------------+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the Dataframe"
      ],
      "metadata": {
        "id": "6oXcNcLikaLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop duplicates and incomplete rows"
      ],
      "metadata": {
        "id": "i8nt2Ez3kX67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.count())\n",
        "df =df.dropna();\n",
        "print(df.count())\n",
        "\n",
        "print(df.count())\n",
        "df=df.drop_duplicates()\n",
        "print(df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFQDW8_qkjoe",
        "outputId": "a4c1bf4b-bf79-43bf-c085-fdc35e75a0a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5033376\n",
            "5033238\n",
            "5033238\n",
            "5033238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print in schema then change string type"
      ],
      "metadata": {
        "id": "ifT_fQrxlzNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KLMCEzcl7e9",
        "outputId": "078654c0-dc52-4507-987f-cf6984aaacd6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- marketplace: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_parent: string (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            " |-- product_category: string (nullable = true)\n",
            " |-- star_rating: string (nullable = true)\n",
            " |-- helpful_votes: string (nullable = true)\n",
            " |-- total_votes: string (nullable = true)\n",
            " |-- vine: string (nullable = true)\n",
            " |-- verified_purchase: string (nullable = true)\n",
            " |-- review_headline: string (nullable = true)\n",
            " |-- review_body: string (nullable = true)\n",
            " |-- review_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType,BooleanType,DateType\n",
        "df =df.withColumn('review_date',col('review_date').cast(DateType()))\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9UfhpdLmJFD",
        "outputId": "5aa9ac7d-f6a8-481a-d1e4-4e8be2c8970d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- marketplace: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_parent: string (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            " |-- product_category: string (nullable = true)\n",
            " |-- star_rating: string (nullable = true)\n",
            " |-- helpful_votes: string (nullable = true)\n",
            " |-- total_votes: string (nullable = true)\n",
            " |-- vine: string (nullable = true)\n",
            " |-- verified_purchase: string (nullable = true)\n",
            " |-- review_headline: string (nullable = true)\n",
            " |-- review_body: string (nullable = true)\n",
            " |-- review_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_id_df= df.select(['review_id','customer_id','product_id','product_parent','review_date'])\n",
        "review_id_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQEJLhxPmbMI",
        "outputId": "3077b6c2-d3c2-4fee-f43e-b7dec75e1859"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------+--------------+-----------+\n",
            "|     review_id|customer_id|product_id|product_parent|review_date|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "|R1004E2Q9GNK0X|   42549410|B005XQLBU4|     464872742| 2015-04-04|\n",
            "|R1007PYPFDDGVX|    6174184|B009KS4XRO|     490050721| 2014-03-05|\n",
            "|R1007WCV4QRLNG|   11902108|B00ATDOVNK|     519315001| 2013-01-26|\n",
            "|R1009WBZG5V78G|   29303557|B00FAI2IUM|     900991309| 2015-07-08|\n",
            "|R100BR0ZNQVPJK|   51124520|B00NG6CIKA|     985585723| 2015-01-26|\n",
            "|R100JN8KBE9F9C|   11416458|B00GRBESP4|     609209651| 2014-08-02|\n",
            "|R100OA8RSE9MFC|   46416338|B00E8KLWB4|     488246640| 2015-07-19|\n",
            "|R100QCVNR5J92R|   27033827|B0097HTPE4|     554042181| 2012-10-21|\n",
            "|R100T4P8XWEGN2|   14091623|B00BIK524A|     454994353| 2014-05-08|\n",
            "|R100UKJQ88UESQ|   42915717|B0052AZJV8|     260966219| 2015-08-16|\n",
            "|R1012OJCOW2YGR|   31030939|B00CPZ7DBW|     788907793| 2014-08-21|\n",
            "|R1016YYC2UMN3U|    1252887|B005ZFOOE8|     447864507| 2014-07-05|\n",
            "|R101B5YVRNYI4I|   10816510|B00A0SQBDM|     972598229| 2014-02-08|\n",
            "|R101LHFD8HJVMM|    8042403|B005ZFOOE8|     447864507| 2015-06-26|\n",
            "|R101TLU75H5F74|    5005229|B00FOYE7IS|     626894118| 2013-12-10|\n",
            "| R101W3VP5Z7K7|   28276880|B004NWQC94|     673194548| 2011-07-06|\n",
            "|R1021IUP1UHV0O|   49283662|B00BJSJKJ4|     657437409| 2013-05-20|\n",
            "|R1022GYB69OY60|   18774316|B00FATEJYE|     716335547| 2015-08-05|\n",
            "|R1029D0RVIEVZ2|   50307879|B005V2S7FW|      60591995| 2015-05-01|\n",
            "|R102O94UKKKNS2|   15129972|B005K17RU0|     423318367| 2012-03-21|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_id_cleaned_df = review_id_df.select(\"review_id\").drop_duplicates()"
      ],
      "metadata": {
        "id": "XUhBSPnpRhWl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_df = df.select(['product_id','product_title'])\n",
        "products_df.show()\n",
        "df.select(['customer_id']).distinct().count()\n",
        "customers_df=df.groupby('customer_id').agg(count('customer_id').alias('customer_count'))\n",
        "customers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTVjFj_NLp5_",
        "outputId": "98c692f6-5aae-4fbe-cee0-01d888ea4917"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|product_id|       product_title|\n",
            "+----------+--------------------+\n",
            "|B005XQLBU4|    Missile Defender|\n",
            "|B009KS4XRO|BINGO BLITZ: Play...|\n",
            "|B00ATDOVNK|Abyss: The Wraith...|\n",
            "|B00FAI2IUM|GT Racing 2: The ...|\n",
            "|B00NG6CIKA|Solitaire TriPeak...|\n",
            "|B00GRBESP4|Just Four Words -...|\n",
            "|B00E8KLWB4|The Secret Societ...|\n",
            "|B0097HTPE4|Word Link - Fun a...|\n",
            "|B00BIK524A|Mahjong Venice My...|\n",
            "|B0052AZJV8|imo free video ca...|\n",
            "|B00CPZ7DBW|     Jump Birdy Jump|\n",
            "|B005ZFOOE8|iHeartRadio – Fre...|\n",
            "|B00A0SQBDM|Nightmares from t...|\n",
            "|B005ZFOOE8|iHeartRadio – Fre...|\n",
            "|B00FOYE7IS|     Guess Zoom Quiz|\n",
            "|B004NWQC94|            Textfree|\n",
            "|B00BJSJKJ4|              Boggle|\n",
            "|B00FATEJYE|Injustice: Gods A...|\n",
            "|B005V2S7FW|            LastPass|\n",
            "|B005K17RU0|AccuWeather with ...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------+--------------+\n",
            "|customer_id|customer_count|\n",
            "+-----------+--------------+\n",
            "|   39329663|            23|\n",
            "|   14054253|             4|\n",
            "|   46765565|             3|\n",
            "|   38013999|             4|\n",
            "|   13073094|             2|\n",
            "|   13092618|             2|\n",
            "|     875164|             1|\n",
            "|   43238024|             9|\n",
            "|   47295518|            13|\n",
            "|   28059157|             1|\n",
            "|   44320490|            11|\n",
            "|     296450|             1|\n",
            "|   18539684|             6|\n",
            "|   31994287|             7|\n",
            "|   19552030|             1|\n",
            "|   39847250|            18|\n",
            "|   21663092|             3|\n",
            "|   15753564|             1|\n",
            "|   51764973|            12|\n",
            "|   26377684|            54|\n",
            "+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df = df.groupby('customer_id').agg(count('customer_id').alias('customer_count'))"
      ],
      "metadata": {
        "id": "5wAUxeKjGICQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKU9Tq8_Gyj-",
        "outputId": "c650cb5f-1cd5-4121-aac0-3d058ae4c942"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|customer_count|\n",
            "+-----------+--------------+\n",
            "|   39329663|            23|\n",
            "|   14054253|             4|\n",
            "|   46765565|             3|\n",
            "|   38013999|             4|\n",
            "|   13073094|             2|\n",
            "|   13092618|             2|\n",
            "|     875164|             1|\n",
            "|   43238024|             9|\n",
            "|   47295518|            13|\n",
            "|   28059157|             1|\n",
            "|   44320490|            11|\n",
            "|     296450|             1|\n",
            "|   18539684|             6|\n",
            "|   31994287|             7|\n",
            "|   19552030|             1|\n",
            "|   39847250|            18|\n",
            "|   21663092|             3|\n",
            "|   15753564|             1|\n",
            "|   51764973|            12|\n",
            "|   26377684|            54|\n",
            "+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vine_df = df.select(['review_id','star_rating','helpful_votes','total_votes','vine'])\n",
        "vine_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bum3WIFrG0E-",
        "outputId": "498e2742-7c72-4acf-dfe3-957dccfa37b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+-------------+-----------+----+\n",
            "|     review_id|star_rating|helpful_votes|total_votes|vine|\n",
            "+--------------+-----------+-------------+-----------+----+\n",
            "|R1004E2Q9GNK0X|          3|            0|          0|   N|\n",
            "|R1007PYPFDDGVX|          4|            0|          0|   N|\n",
            "|R1007WCV4QRLNG|          5|            0|          0|   N|\n",
            "|R1009WBZG5V78G|          5|            1|          1|   N|\n",
            "|R100BR0ZNQVPJK|          4|            0|          0|   N|\n",
            "|R100JN8KBE9F9C|          3|            0|          0|   N|\n",
            "|R100OA8RSE9MFC|          4|            0|          0|   N|\n",
            "|R100QCVNR5J92R|          5|            2|          3|   N|\n",
            "|R100T4P8XWEGN2|          5|            0|          0|   N|\n",
            "|R100UKJQ88UESQ|          1|            0|          0|   N|\n",
            "|R1012OJCOW2YGR|          4|            0|          0|   N|\n",
            "|R1016YYC2UMN3U|          5|            0|          1|   N|\n",
            "|R101B5YVRNYI4I|          4|            0|          0|   N|\n",
            "|R101LHFD8HJVMM|          4|            0|          0|   N|\n",
            "|R101TLU75H5F74|          5|            0|          0|   N|\n",
            "| R101W3VP5Z7K7|          5|            5|          5|   N|\n",
            "|R1021IUP1UHV0O|          3|            0|          0|   N|\n",
            "|R1022GYB69OY60|          2|            0|          0|   N|\n",
            "|R1029D0RVIEVZ2|          5|            0|          0|   N|\n",
            "|R102O94UKKKNS2|          5|            0|          0|   N|\n",
            "+--------------+-----------+-------------+-----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "scnx8C8rLTW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure settings for RDS"
      ],
      "metadata": {
        "id": "3A9Dw9ncLWvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure settings for RDS--- # Write DataFrame to each table in RDS"
      ],
      "metadata": {
        "id": "KFlyWA7ULbyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Protect password\n",
        "from getpass import getpass\n",
        "password = getpass('password')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2piZG5qD3Ws",
        "outputId": "fdfff7c1-4769-41e8-f911-4dac61a2107b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "password··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://mydbinstance.ccn4uoh5hvud.us-east-1.rds.amazonaws.com:5432/mobile_apps_db\"\n",
        "configuration = {\"user\": \"postgres\", \n",
        "          \"password\": password, \n",
        "          \"driver\": \"org.postgresql.Driver\"}"
      ],
      "metadata": {
        "id": "5qUlALXrD5_m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=configuration)"
      ],
      "metadata": {
        "id": "x9qqZlt5D6zk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=configuration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uU6m_EqvEYj9",
        "outputId": "292a60bf-0591-40ab-e890-1ed5551ab0d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-736225a88543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproducts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'products'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o109.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 1224, 3c47a6bddd3c, executor driver): java.sql.BatchUpdateException: Batch entry 13 INSERT INTO products (\"product_id\",\"product_title\") VALUES ('B005ZFOOE8','iHeartRadio – Free Music & Internet Radio') was aborted: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B005ZFOOE8) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1357)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1382)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:494)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:699)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:871)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:869)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:994)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:994)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B005ZFOOE8) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:994)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:992)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:869)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:790)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: Batch entry 13 INSERT INTO products (\"product_id\",\"product_title\") VALUES ('B005ZFOOE8','iHeartRadio – Free Music & Internet Radio') was aborted: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B005ZFOOE8) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1357)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1382)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:494)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:699)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:871)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:869)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:994)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:994)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B005ZFOOE8) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n\t... 20 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=configuration)"
      ],
      "metadata": {
        "id": "yUZS78sIEZNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vine_table_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=configuration)"
      ],
      "metadata": {
        "id": "Qeh-94HuEa9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}